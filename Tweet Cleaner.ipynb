{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e21aaf",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa893c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f732f",
   "metadata": {},
   "source": [
    "# Download the Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b676f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the API to interact with Kaggle\n",
    "api = KaggleApi()\n",
    "#Authenticate using API credentials\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428e6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the competition files\n",
    "api.competition_download_files('nlp-getting-started')\n",
    "#Open the zip file\n",
    "zf = ZipFile('nlp-getting-started.zip')\n",
    "#Extract the files and specify the location\n",
    "zf.extractall(\"data\\\\\") \n",
    "#Close the zip file\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c525a",
   "metadata": {},
   "source": [
    "# Read in the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c946d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2ca05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29a038ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train.drop([\"id\", \"keyword\", \"location\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3035923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c77fd1",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b99c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    \"\"\"\n",
    "    This function removes unnecessary HTML tags in the corpus. \n",
    "    \n",
    "    test: corpus of text data\n",
    "    \n",
    "    returns: text with no HTML\n",
    "\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup([\"iframe\", \"script\"])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    \n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e935aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(text):\n",
    "    \"\"\"\n",
    "    This function takes a corpus and tokenizes it into sentences\n",
    "    Parameters\n",
    "    \n",
    "    test: String containing the corpus\n",
    "    \n",
    "    returns: Corpus of tokenized sentences\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.sent_tokenize\n",
    "    sentence_tokens = tokenizer(text=text)\n",
    "    return sentence_tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d113d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    \"\"\"\n",
    "    This function takes a corpus and splits the sentences into words\n",
    "    \n",
    "    test: String of tokenized sentences\n",
    "    \n",
    "    returns: Array of tokenized words\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.word_tokenize\n",
    "    word_tokens = tokenizer(text)\n",
    "    return np.array(word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf3dad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    \"\"\"\n",
    "    This function removes accented characters from the corpus using ASCII\n",
    "    characters\n",
    "    \n",
    "    text: String corpus of text data\n",
    "    \n",
    "    returns: Corpus with all characters converted and standardized into ASCII characters\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce401c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This map was gotten from the internet\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping = CONTRACTION_MAP):\n",
    "    \"\"\"\n",
    "    This function takes a corpus of text data and expands all contractions\n",
    "    bassed on the contraction mapping\n",
    "    \n",
    "    text: String corpus of text data\n",
    "    \n",
    "    returns: Text corpus with all contractions expanded\n",
    "\n",
    "    \"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                    else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d97cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits = False):\n",
    "    \"\"\"\n",
    "    This function returns the text corpus with all special characters removed with the option\n",
    "    to remove digits\n",
    "    \n",
    "    text: String corpus\n",
    "    \n",
    "    returns: Text corpus with special characters removed\n",
    "\n",
    "    \"\"\"\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3607eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_conversation(text, text_lower = True):\n",
    "    \"\"\"\n",
    "    This function returns the corpus with all tokens lowercase or uppercase\n",
    "    \n",
    "    text: String corpus\n",
    "    text_lower: Boolen, default is True\n",
    "    \n",
    "    returns: Text corpus with all tokens converted\n",
    "\n",
    "    \"\"\"\n",
    "    if text_lower:\n",
    "        return text.lower()\n",
    "    else:\n",
    "        return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8241222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    This function returns the lemmatized text corpus. Gives correct spelling. \n",
    "\n",
    "    text: String corpus\n",
    "    \n",
    "    returns: String of lemmatized text corpus\n",
    "    \"\"\"\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5245bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    \"\"\"\n",
    "    This function removes all the stopwords from the text corpus based on the pre-existing \n",
    "    list of stopwords from nltk. \n",
    "    \n",
    "    text: String corpus\n",
    "    is_lower_case: Boolean to make the corpus lower case\n",
    "    \n",
    "    returns: String corpus with stopwords removed\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "        \n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07782cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping = True, contraction_expansion = True, accented_char_removal = True, \n",
    "                     text_lower_case = True, text_lemmatization = True, special_char_removal = True, stopword_removal=True, \n",
    "                     remove_digits = True):\n",
    "    \"\"\"\n",
    "    This function normalizes the text corpus based on all the functions above. \n",
    "\n",
    "    corpus : Text corpus\n",
    "    html_stripping : boolean, optional\n",
    "        Controls if the text corpus is stripped of htmls\n",
    "    contraction_expansion : boolean, optional\n",
    "        Controls if the text corpus has its contractions extraction\n",
    "    accented_char_removal : boolean, optional\n",
    "        Controls if the text corpus has its accented characters removed\n",
    "    text_lower_case : boolean, optional\n",
    "        Controls if the text corpus has its text changed to lower case\n",
    "    correct_spelling : boolean, optional\n",
    "        Controls if the text corpus has its spelling corrected\n",
    "    text_lemmatization : boolean, optional\n",
    "        Controls if the text corpus his its words lemmatized\n",
    "    special_char_removal : boolean, optional\n",
    "        Controls if the text corpus has its special characters removed\n",
    "    stopword_removal : boolean, optional\n",
    "        Controls if the text corpus has its stopwords removed\n",
    "    remove_digits : boolean, optional\n",
    "        Controls if the text corpus has its digits removed\n",
    "\n",
    "    returns: String of normalized text corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized_corpus = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        \n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "            \n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "            \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "            \n",
    "        if text_lower_case:\n",
    "            doc = case_conversation(doc)\n",
    "            \n",
    "        #Remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n",
    "            \n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize(doc)\n",
    "            \n",
    "        if special_char_removal:\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "            \n",
    "        #Removes extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        \n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2f323",
   "metadata": {},
   "source": [
    "### Normalize the Corpus of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2491425e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stopword_list.remove(\"no\")\n",
    "stopword_list.remove(\"not\")\n",
    "\n",
    "corpus = tweets_train[\"text\"]\n",
    "norm_corpus = np.array(normalize_corpus(corpus))\n",
    "\n",
    "tweets_train[\"Clean Text\"] = norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "698d401b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Clean Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer no evacuation shelter place order expect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>rockyfire update california hwy close direction due lake county fire cafire wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas</td>\n",
       "      <td>1</td>\n",
       "      <td>flood disaster heavy rain cause flash flooding street manitou colorado spring area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in the woods...</td>\n",
       "      <td>1</td>\n",
       "      <td>I top hill I see fire wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's an emergency evacuation happening now in the building across the street</td>\n",
       "      <td>1</td>\n",
       "      <td>emergency evacuation happen building across street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm afraid that the tornado is coming to our area...</td>\n",
       "      <td>1</td>\n",
       "      <td>I afraid tornado come area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "5  #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires                          \n",
       "6  #flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas                                         \n",
       "7  I'm on top of the hill and I can see a fire in the woods...                                                                             \n",
       "8  There's an emergency evacuation happening now in the building across the street                                                         \n",
       "9  I'm afraid that the tornado is coming to our area...                                                                                    \n",
       "\n",
       "   target  \\\n",
       "0  1        \n",
       "1  1        \n",
       "2  1        \n",
       "3  1        \n",
       "4  1        \n",
       "5  1        \n",
       "6  1        \n",
       "7  1        \n",
       "8  1        \n",
       "9  1        \n",
       "\n",
       "                                                                             Clean Text  \n",
       "0  deed reason earthquake may allah forgive                                              \n",
       "1  forest fire near la ronge sask canada                                                 \n",
       "2  resident ask shelter place notify officer no evacuation shelter place order expect    \n",
       "3  people receive wildfire evacuation order california                                   \n",
       "4  got send photo ruby alaska smoke wildfire pour school                                 \n",
       "5  rockyfire update california hwy close direction due lake county fire cafire wildfire  \n",
       "6  flood disaster heavy rain cause flash flooding street manitou colorado spring area    \n",
       "7  I top hill I see fire wood                                                            \n",
       "8  emergency evacuation happen building across street                                    \n",
       "9  I afraid tornado come area                                                            "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 2)\n",
    "tweets_train.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea610472",
   "metadata": {},
   "source": [
    "Check if normalizing resulted in any rows having a missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43ed86c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          0\n",
       "target        0\n",
       "Clean Text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = tweets_train.replace(r'^(\\s)+$', np.nan, regex = True)\n",
    "tweets_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b53fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
