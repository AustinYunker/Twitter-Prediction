{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a64f61",
   "metadata": {},
   "source": [
    "# Setups and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5605cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "sns.set\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import Tweet_Normalizer as tn\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import spacy\n",
    "import scipy\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import csv\n",
    "import math\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92508e1d",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cecd4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482157dd",
   "metadata": {},
   "source": [
    "# Clean and Normalize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2782cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tweet scrubber...\n",
      "\n",
      "Dropping unnecessary columns\n",
      "Successfully dropped columns!\n",
      "\n",
      "Normalizing the tweets\n",
      "Successfully normalized tweets!\n",
      "\n",
      "Removing invalid and mispelled words\n",
      "Successfully removed invalid and mispelled words!\n",
      "\n",
      "Successfully scrubbed tweets!\n",
      "\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#USe tweet scrubber function to clean the data\n",
    "tweets = tn.tweet_scrubber(tweets, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde12cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for blank rows after cleaning. We expect 5\n",
    "tweets = tweets.replace(r'^(\\s)+$', np.nan, regex = True)\n",
    "#Drop the empty rows\n",
    "tweets.dropna(subset=[\"Clean Tweets\"], inplace = True)\n",
    "#Reset the index in place\n",
    "tweets.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd99e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the last few rows of the data\n",
    "tweets.tail(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b42774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
