{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0b6d6e",
   "metadata": {},
   "source": [
    "# Setups and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e9591ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "sns.set\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import Tweet_Normalizer as tn\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import spacy\n",
    "import scipy\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import csv\n",
    "import math\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92508e1d",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9255bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482157dd",
   "metadata": {},
   "source": [
    "# Clean and Normalize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9b0945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tweet scrubber...\n",
      "\n",
      "Dropping unnecessary columns\n",
      "Successfully dropped columns!\n",
      "\n",
      "Normalizing the tweets\n",
      "Successfully normalized tweets!\n",
      "\n",
      "Removing invalid and mispelled words\n",
      "Successfully removed invalid and mispelled words!\n",
      "\n",
      "Successfully scrubbed tweets!\n",
      "\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#USe tweet scrubber function to clean the data\n",
    "tweets = tn.tweet_scrubber(tweets, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36023e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for blank rows after cleaning. We expect 5\n",
    "tweets = tweets.replace(r'^(\\s)+$', np.nan, regex = True)\n",
    "#Drop the empty rows\n",
    "tweets.dropna(subset=[\"Clean Tweets\"], inplace = True)\n",
    "#Reset the index in place\n",
    "tweets.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d015fd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Clean Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7593</th>\n",
       "      <td>Father-of-three Lost Control of Car After Overtaking and Collided #BathAndNorthEastSomerset http://t.co/fa3FcnlN86</td>\n",
       "      <td>1</td>\n",
       "      <td>father three lose control car overtake collide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>1.3 #Earthquake in 9Km Ssw Of Anza California #iPhone users download the Earthquake app for more information http://t.co/V3aZWOAmzK</td>\n",
       "      <td>1</td>\n",
       "      <td>earthquake km ssw  california  user download earthquake  information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>Evacuation order lifted for town of Roosevelt: http://t.co/EDyfo6E2PU http://t.co/M5KxLPKFA1</td>\n",
       "      <td>1</td>\n",
       "      <td>evacuation order lift town roosevelt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>#breaking #LA Refugio oil spill may have been costlier bigger than projected http://t.co/5ueCmcv2Pk</td>\n",
       "      <td>1</td>\n",
       "      <td>break la  oil spill may costlier big project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>a siren just went off and it wasn't the Forney tornado warning ??</td>\n",
       "      <td>1</td>\n",
       "      <td>siren go not  tornado warning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>Officials say a quarantine is in place at an Alabama home over a possible Ebola case after developing symptoms... http://t.co/rqKK15uhEY</td>\n",
       "      <td>1</td>\n",
       "      <td>official say quarantine place alabama home possible ebola case develop symptom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: UPDATE: FIRE crews have evacuated up to 30 passengers who were tr... http://t.co/EYSVvzA7Qm</td>\n",
       "      <td>1</td>\n",
       "      <td>fall  g link tram update fire crew evacuate passenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>on the flip side I'm at Walmart and there is a bomb and everyone had to evacuate so stay tuned if I blow up or not</td>\n",
       "      <td>1</td>\n",
       "      <td>flip side I  bomb  evacuate stay tune I blow not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7601</th>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site mosque - Reuters via World - Google News - Wall ... http://t.co/nF4IculOje</td>\n",
       "      <td>1</td>\n",
       "      <td>suicide bomber kill saudi security site mosque   world google news wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... - http://t.co/3SICroAaNz http://t.co/I27Oa0HISp</td>\n",
       "      <td>1</td>\n",
       "      <td>violent record break  el reno oklahoma tornado nearly run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold bridge collapse nearby home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.</td>\n",
       "      <td>1</td>\n",
       "      <td>control wild fire california even northern part state troubling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ</td>\n",
       "      <td>1</td>\n",
       "      <td>[ utc ] km volcano hawaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.</td>\n",
       "      <td>1</td>\n",
       "      <td>police investigate e bike collide car little portugal e bike rider suffer serious non life threaten injury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d</td>\n",
       "      <td>1</td>\n",
       "      <td>late home raze northern california wildfire abc news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "7593  Father-of-three Lost Control of Car After Overtaking and Collided #BathAndNorthEastSomerset http://t.co/fa3FcnlN86                          \n",
       "7594  1.3 #Earthquake in 9Km Ssw Of Anza California #iPhone users download the Earthquake app for more information http://t.co/V3aZWOAmzK         \n",
       "7595  Evacuation order lifted for town of Roosevelt: http://t.co/EDyfo6E2PU http://t.co/M5KxLPKFA1                                                \n",
       "7596  #breaking #LA Refugio oil spill may have been costlier bigger than projected http://t.co/5ueCmcv2Pk                                         \n",
       "7597  a siren just went off and it wasn't the Forney tornado warning ??                                                                           \n",
       "7598  Officials say a quarantine is in place at an Alabama home over a possible Ebola case after developing symptoms... http://t.co/rqKK15uhEY    \n",
       "7599  #WorldNews Fallen powerlines on G:link tram: UPDATE: FIRE crews have evacuated up to 30 passengers who were tr... http://t.co/EYSVvzA7Qm    \n",
       "7600  on the flip side I'm at Walmart and there is a bomb and everyone had to evacuate so stay tuned if I blow up or not                          \n",
       "7601  Suicide bomber kills 15 in Saudi security site mosque - Reuters via World - Google News - Wall ... http://t.co/nF4IculOje                   \n",
       "7602  #stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... - http://t.co/3SICroAaNz http://t.co/I27Oa0HISp      \n",
       "7603  Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5                                                         \n",
       "7604  @aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.               \n",
       "7605  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ                                                                           \n",
       "7606  Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.   \n",
       "7607  The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d                                              \n",
       "\n",
       "      target  \\\n",
       "7593  1        \n",
       "7594  1        \n",
       "7595  1        \n",
       "7596  1        \n",
       "7597  1        \n",
       "7598  1        \n",
       "7599  1        \n",
       "7600  1        \n",
       "7601  1        \n",
       "7602  1        \n",
       "7603  1        \n",
       "7604  1        \n",
       "7605  1        \n",
       "7606  1        \n",
       "7607  1        \n",
       "\n",
       "                                                                                                    Clean Tweets  \n",
       "7593  father three lose control car overtake collide                                                              \n",
       "7594  earthquake km ssw  california  user download earthquake  information                                        \n",
       "7595  evacuation order lift town roosevelt                                                                        \n",
       "7596  break la  oil spill may costlier big project                                                                \n",
       "7597  siren go not  tornado warning                                                                               \n",
       "7598  official say quarantine place alabama home possible ebola case develop symptom                              \n",
       "7599   fall  g link tram update fire crew evacuate passenger                                                      \n",
       "7600  flip side I  bomb  evacuate stay tune I blow not                                                            \n",
       "7601  suicide bomber kill saudi security site mosque   world google news wall                                     \n",
       "7602   violent record break  el reno oklahoma tornado nearly run                                                  \n",
       "7603  two giant crane hold bridge collapse nearby home                                                            \n",
       "7604    control wild fire california even northern part state troubling                                           \n",
       "7605  [ utc ] km volcano hawaii                                                                                   \n",
       "7606  police investigate e bike collide car little portugal e bike rider suffer serious non life threaten injury  \n",
       "7607  late home raze northern california wildfire abc news                                                        "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the last few rows of the data\n",
    "pd.set_option('display.max_colwidth', 2)\n",
    "tweets.tail(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb86f2",
   "metadata": {},
   "source": [
    "# Split the Data intro Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a59ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, val_corpus, y_train, y_val = train_test_split(tweets[\"Clean Tweets\"], np.array(tweets[\"target\"]), \n",
    "                                                  test_size=.15, random_state=42, stratify=np.array(tweets[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03b229df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vectorizer(corpus, model, num_features):\n",
    "    \"\"\"\n",
    "    This averages all the word embeddings in the tweet. \n",
    "    This function averages all the word embeddings in the tweet.\n",
    "    \n",
    "    corpus: String text corpus\n",
    "    Model: Model to use\n",
    "    num_features: Int, the number of features to use\n",
    "    \n",
    "    \"\"\"\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                nwords += 1\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "            \n",
    "        return feature_vector\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                for tokenized_sentence in corpus]\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "759768f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the training and validation set\n",
    "tokenizer = ToktokTokenizer()\n",
    "tokenized_train = [tokenizer.tokenize(text) for text in train_corpus]\n",
    "tokenized_val = [tokenizer.tokenize(text) for text in val_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8157fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb16e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of features to use\n",
    "w2v_num_features = 300\n",
    "#Create the Word2Vec model\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, vector_size=w2v_num_features,\n",
    "                                   window = 250, epochs=100, min_count=0, sample=1e-3,\n",
    "                                   sg=1, workers=10)\n",
    "#Creat the training data\n",
    "X_train = document_vectorizer(corpus=train_corpus, model=w2v_model, \n",
    "                                            num_features=w2v_num_features)\n",
    "\n",
    "#Create the test data\n",
    "X_val = document_vectorizer(corpus=val_corpus, model=w2v_model, \n",
    "                                            num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "457eb19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6466, 300)\n",
      "(1142, 300)\n",
      "(6466,)\n",
      "(1142,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fed817",
   "metadata": {},
   "source": [
    "# Baseline Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7ff270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bd65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_pred = cross_val_predict(lr_clf, X_train, y_train, cv = 2)\n",
    "lr_base_acc = accuracy_score(y_train, y_train_pred) * 100\n",
    "lr_base_f1 = f1_score(y_train, y_train_pred) * 100\n",
    "print(f\"Logistic Regression Baseline Accuracy: {lr_base_acc:.2f}\")\n",
    "print(f\"Logistic Regression Baseline F1-Score: {lr_base_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(log_reg, X_train, y_train, cv = 5)\n",
    "lr_cv_f1 = np.round(f1_score(y_train, y_train_pred), 4) * 100\n",
    "lr_cv_acc = np.round(accuracy_score(y_train, y_train_pred), 4) * 100\n",
    "print(f'Logistic Regression 5-fold CV Baseline F1-Score: {lr_cv_f1:.2f}%')\n",
    "print(f'Logistic Regression 5-fold CV Baseline Accuracy: {lr_cv_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
